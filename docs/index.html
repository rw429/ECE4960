<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Fast Robots - ECE 4960</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
        <script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg bg-secondary text-uppercase fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top">ECE 4960 - Robert Whitney</a>
                <button class="navbar-toggler navbar-toggler-right text-uppercase font-weight-bold bg-primary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#portfolio">Labs</a></li>
                        <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#about">About Me</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <header class="masthead bg-primary text-white text-center">
            <div class="container d-flex align-items-center flex-column">
                <!-- Masthead Avatar Image-->
<!--                 <img class="masthead-avatar mb-5" src="assets/img/avataaars.svg" alt="" /> -->
                <!-- Masthead Heading-->
                <h1 class="masthead-heading text-uppercase mb-0">Fast Robots - ECE 4960</h1>
                <!-- Icon Divider-->
                <div class="divider-custom divider-light">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- Masthead Subheading-->
                <p class="masthead-subheading font-weight-light mb-0">Robert Whitney</p>
            </div>
        </header>
        <!-- Portfolio Section-->
        <section class="page-section portfolio" id="portfolio">
            <div class="container">
                <!-- Portfolio Section Heading-->
                <h2 class="page-section-heading text-center text-uppercase text-secondary mb-0">Labs</h2>
                <!-- Icon Divider-->
                <div class="divider-custom">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- Portfolio Grid Items-->
                <div class="row">
                    <!-- Portfolio Item 1-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal1">
                            <h2 class="text-warning">1: Introduction to Artemis</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/lab1/artemis.jpg" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 2-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal2">
                            <h2 class="text-warning">2: Bluetooth Communication</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/cake.png" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 3-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal3">
                            <h2 class="text-warning">3: Characterizing the Car</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/lab3/car_wBody.jpg" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 4-->
                    <div class="col-md-6 col-lg-4 mb-5 mb-lg-0">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal4">
                            <h2 class="text-warning">4: Open-Loop Control</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/lab4/assembled.jpg" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 5-->
                    <div class="col-md-6 col-lg-4 mb-5 mb-md-0">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal5">
                            <h2 class="text-warning">5: Obstacle Avoidance</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/lab5/robot.jpg" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 6-->
                    <div class="col-md-6 col-lg-4">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal6">
                            <h2 class="text-warning">6: IMU and Odometry</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/lab6/robotSquare.jpg" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 7-->
                    <div class="col-md-6 col-lg-4">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal7">
                            <h2 class="text-warning">7: Mapping</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/lab7/kitchen.jpg" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 8-->
                    <div class="col-md-6 col-lg-4">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal8">
                            <h2 class="text-warning">8: Localization in Simulation</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/lab8/Bayes.PNG" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 9-->
                    <div class="col-md-6 col-lg-4">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal9">
                            <h2 class="text-warning">9: Localization</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/cabin.png" alt="" />
                        </div>
                    </div>
                    <!-- Portfolio Item 10-->
                    <div class="col-md-6 col-lg-4">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal10">
                            <h2 class="text-warning">10: Path Planning and Execution</h2>
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div>
                            <img class="img-fluid" src="assets/img/portfolio/cabin.png" alt="" />
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- About Section-->
        <section class="page-section bg-primary text-white mb-0" id="about">
            <div class="container">
                <!-- About Section Heading-->
                <h2 class="page-section-heading text-center text-uppercase text-white">About Me</h2>
                <!-- Icon Divider-->
                <div class="divider-custom divider-light">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- About Section Content-->
                <div class="row">
                    <div class="col-lg-4 ml-auto"><p class="lead">Welcome to my Fast Robots page! I'm a Master of Engineering student in the Cornell University Graduate School.  I've always been fascinated by robots, airplanes, rocketships, drones, anything that moves fast!  Unsurprisingly, I'm thrilled to be taking this class where I will develop an autonomous RC car for localization, mapping, path planning, and some cool stunts too.</p></div>
                    <div class="col-lg-4 mr-auto"><img class="img-fluid" src="assets/img/portfolio/headshot.jpg" alt="" /></div>
                </div>
                <!-- About Section Button-->
                <div class="text-center mt-4">
                    <a class="btn btn-xl btn-outline-light" href="https://rw429.github.io/Robert-Whitney-Portfolio/">
                        <i class="fas fa-download mr-2"></i>
                        Check out my Portfolio
                    </a>
                </div>
            </div>
        </section>
        <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes)-->
        <div class="scroll-to-top d-lg-none position-fixed">
            <a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top"><i class="fa fa-chevron-up"></i></a>
        </div>
        <!-- Portfolio Modals-->
        <!-- Portfolio Modal 1-->
        <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-labelledby="portfolioModal1Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal1Label">Lab 1: Introduction to Artemis</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab1/artemis.jpg" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="mb-5">The purpose of this lab is to setup and become familiar with the Arduino IDE and the Artemis board. After this lab, I will be able to program my board, blink the LED, read/write serial messages over USB, display the output from the onboard temperature sensor, measure the loudest frequency recorded by the Pulse Density Microphone, and run the board using a battery instead of my computer.</p>
                                    <p class="mb-5">The first part of lab 1 was simple because I already have the Arduino IDE installed on my computer and am familiar with how these devices work.  I uploaded the “Blink” sketch from the examples folder to ensure that sketches could be uploaded to the Artemis.  From previous experience, I know that it is successful when the RX and TX lights on the Artemis blink rapidly while the sketch is uploading.  To double check that the code was working properly, I changed the timing of the blinking, uploaded the new sketch, and observed the light blinking faster as expected.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab1/blink.PNG">
                                    <p class="mb-5">The “Serial” example worked as expected, parsing inputs to the Artemis and using those inputs to display messages on the Serial Monitor.  I had to change the baud rate to view them properly.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab1/serial.PNG">
                                    <p class="mb-5">The “analogRead” example worked as well, although the functionality of the sketch was not fully realized.  The temperature sensor reported gradually higher temperatures as I held my thumb to the board.  However, the pin being read as an analog input was floating as it was not grounded or connected to an input, as expected.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab1/temp1.PNG">
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab1/temp2.PNG">
                                    <p class="mb-5">The “microphoneOutput” example sketch also worked as described, returning the frequency of the loudest sound.  When whistling loudly, I could change the output on the serial monitor by changing the pitch of my whistling in a range from 100 to 2500.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab1/mic1.PNG">
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab1/mic2.PNG">
                                    <p class="mb-5">To test the battery, I inserted an “if” statement to turn on the onboard LED (pin 19) if the loudest frequency was above 2000 and turn it off otherwise.  I also had to set pin 19 as an output in the setup function.  This modification to the sketch worked as expected.  I could whistle a low tune while watching the serial monitor, slowly raise the output above 2000, and watch the LED turn on.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab1/mic3.PNG">
                                    <p class="mb-5">I also unplugged the Artemis from the USB cable and plugged in the battery.  The power LED turned on, indicating the sketch was probably running.  I tested the sketch with whistling again, this time with no serial monitor to check the frequency.  Again, the LED turned on when I whistled a high enough frequency and turned off when it was below the threshold.</p>
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 2-->
        <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-labelledby="portfolioModal2Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal2Label">Lab 2: Bluetooth Communication</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/cake.png" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="text-left">The Artemis board and a computer are capable of low-latency, moderate-throughput wireless communication via Bluetooth LE. In this lab, you will expand a basic application for wireless communication over a Generic Attribute (GATT) framework on top of robust BLE stacks.</p>
                                    <h3>Ping Your Robot</h3>
                                    <p class="text-left">After getting a reliable bluetooth connection, the round-trip latency of the link was tested.  After changing a few lines in main.py and running it, the VM terminal output the response time for sending messages to the Artemis board over bluetooth.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab2/pongOutput.png">
                                    <p class="text-left">The latency of the wireless messages was most commonly between 180 and 210 ms, although some bytes were sent and received in as little as 160 ms and as much as 250 ms.</p>
                                    <p class="text-left">To test the latency of a wired serial connection, a short script was written in Arduino and another in Matlab.  The Arduino script waits to receive a 'ping' char over a serial port set at 115200 baud/s before sending a 'pong' char in response.
                                    <br><br><code>
                                    while (Serial.available()) {                        //wait for ping<br>
                                        if (Serial.read() == 'p') Serial.println('r');  //send response<br>
                                    }</code></p>
                                    <p class = "text-left"> The Matlab script logs the time between sending the initial 'ping' and finishing parsing the response from the Artemis.
                                    <br><br><code>
                                    write(s,'p','char');    %send char<br>
                                    tic                     %start timer<br>
                                    msg = readline(s);      %wait for response and read it<br>
                                    tResponse(i) = toc;     %stop timer and save<br></code> </p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab2/pongOutputWired.png">
                                    <p class="text-left">The latency of a wired serial connection over 1000 trials was significantly lower compared to that of the Bluetooth connection.  The round-trip latency for sending a single char and then parsing the response averaged 4ms, though it must be noted that only one char was sent at a time.</p>
                                    <p class="text-left">To check how many bytes are in the data packet, <code>Serial.println(l_Rcvd);</code> was added in the Arduino script after the other print statements.  <code>l_Rcvd</code> is the total length of the packet we are sending in bytes.  It continuously printed 99, indicating the ping-pong message was sending a full packet back and forth every time.</p>
                                    <h3>Requesting a Float</h3>
                                    <p class="text-left">Next, I used the same data structure to request a float with the VM and send a response with the Artemis.  For the VM, I simply uncommented <code>await theRobot.sendCommand(Commands.REQ_FLOAT)</code>.  For the Arduino script, I had to insert a float (I chose pi to 7 significant digits) into <code>res_cmd->data</code> as well as specify <code>res_cmd->command_type</code> and the total number of bytes in the packet (1 command byte + 1 length byte + 4 float bytes = 6 total bytes).</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab2/reqFloat.PNG">
                                    <p class="text-left">After running both of these programs, the VM terminal output "3.141592502593994" instead of the float that I sent which was "3.141596".  They are as close as they can be for 4 byte floats without being equal.</p>
                                    <h3>Testing the Data Rate</h3>
                                    <p class="text-left">Last, the data rate was tested by sending 32 and 64 bit integers to the VM.  The data structure was packed with either a uint32 or a uint64 then sent off while keeping track of the time in between.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab2/streamArduino.PNG">
                                    <p class="text-left">In the VM program, I only had to uncomment <code>await theRobot.testByteStream(25)</code>.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab2/streamArduinoHist.png">
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab2/streamArduinoHist64.png">
                                    <p class="text-left">The Artemis sent packets averaging about every 10.7ms regardless of if they were uint32 or uint64.</p>
                                    <h3>Conclusion</h3>
                                    <p class="text-left">After completing this lab, I see the role that this bluetooth capability will play as the project progresses.  The latency and data rate of the connection is far too slow for a fast robot to react dynamically using the VM for the majority of its calculations.  For example, I expect that for the inverted pendulum lab to work, the Artemis will have to sense, compute, and actuate 20-100 times per second to achieve closed-loop stability, an update frequency too fast to be doing much computation over bluetooth.  Thus, the Artemis and the VM must work together and compensate for eachothers weaknesses to solve whatever task is issued.</p>
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 3-->
        <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-labelledby="portfolioModal3Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal3Label">Lab 3: Characterizing the Car</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab3/car_wBody.jpg" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="text-left">Before we start taking the car apart, we can leverage the remote control to familiarize ourselves with the capabilities of the hardware. In later labs we will seek to replicate manual behavior with autonomous control and in some cases exceed what manual control can make it do for planar navigation and more dynamic (‘flippy’) behaviors. This is a fairly open ended lab, but the goal is to methodically document the car in any way that might be useful later on.</p><br>
                                    <h3>Testing</h3>
                                    <p class="text-left">There are important physical charactersitcs of the car that will be essential for any model of that we intend to implement in future labs.  A more complete model accelerates iterative testing and will simplify future labs.</p>
                                    <p class="text-left">I started with determining the relevant dimensions with electronic calipers.  First, I removed the plastic top as I expected it would get in the way of future labs.  The wheelbase (front to rear tires) is W = 80mm and the track (left to right tires) is T = 105mm.  The diameter of each tire is D = 77mm, although this may change with loading as the wheels are deformable.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab3/topDown_carMarked.jpg">
                                    <p class="text-left">The front and rear axles are equally far (40mm) from the geometric center of the body of the car whose length, width, and height are 119mm, 65mm, and 45mm respectively.  </p>
                                    <p class="text-left">The battery life lasts about 20 minutes of continuous use involving accelerating, braking, spinning in place, doing tricks, and just moving around in general.  I consider the battery "fully exhausted" when the robot can no motors can no longer overcome the rolling resistance on carpet to move.  Each battery averages 4 hours to charge from dead (4.8V) to fully charged (5.6V).</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab3/multimeter.jpg">
                                    <p class="text-left">The performance of the car is dependent on the surface it is driving on, the driving mode (fast or slow mode), and the battery.  On tile and hardwood, the car has no problem accelerating, turning in place, and doing tricks, even on low battery.  However, the car struggles to overcome friction when on carpet.  Even on a full battery, turning in place is not possible so a K-turn maneuver is required to change the heading.</p>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/-tvlwy7Hqs4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">The car can do a few cool stunts when it has enough battery.  By accelerating or decelerating very suddenly, I can flip the car over itself.  I am unable to balance it on two wheels, but I can get it to flip and stay up for an instant.  Additionally, I can somewhat reliably get the car to stop inches infront of a wall starting from full speed.</p>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/VX6inwBH2fs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <br>
                                    <br>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/_gmS72GifNQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">Here is a video of the car doing several very fast flips at full throttle.</p>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/oNSitK4q3H8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">And here is a video of me trying to hit 4 targets as quickly as I can and return to the startpoint.</p>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/DdFMu0o7qcQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">I performed several tests to get a rough estimate of the speed, acceleration, and braking distance.  All tests were performed with a full battery on the tiled floor where I expect to be running the robot for the remaining labs.  A slow motion camera recorded a stopwatch and several trials of the robot driving across the 12" x 12" tiles.  Speed was tested by getting the car up to speed and recording the time for the car to cross 4 tiles (4 feet).  The average speed while in fast and slow mode over 5 trials was 8.85 ft/s (2.69 m/s) and 6.50 ft/s (1.98 m/s) respectively.  Acceleration was tested by recording the time it takes for the car to cross 1 tile, starting from rest and assuming constant acceleration equal to a = 2d/t^2 where d is the distance traveled in time t.  The average acceleration while in fast and slow mode over 5 trials was 20.0 ft/s^2 (6.10 m/s^2) and 6.49 ft/s^2 (1.98 m/s^2) respectively. I have included a video showing how I did the acceleration testing below.  I was surprised at the level of repeatability of these tests, although I would expect very different numbers if I used a battery closer to empty.</p>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/CT3kTQUzG1U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">I performed another 2 tests for braking. First, I let the car get up to speed in fast mode, then switched to slow mode upon reversing the throttle on both sets of wheels.  Next, I performed the same test, but I left the car get up to speed in slow mode before reversing the throttle.  The first and second tests ranged from 2-3 feet and 1-1.25 feet between initiating braking and coming to rest respectively. The repeatability of the braking tests did not compare to that of the speed and acceleration tests.  The reason for the discrepancy is the difficulty in reversing the throttle at the exact moment the car would begin to cross a marked tile.</p>
                                    <h3>Simulator Testing</h3>
                                    <p class="text-left">The simulator will be critical for iterative testing in future labs.  We can use it to quickly test path planning, mapping, and localization tools before implementing them on the real robot.  After being setup in the VM, some tests were performed to check the basic functionality of the simulator.</p>
                                    <p class="text-left">The robot can be controlled using the keyboard teleoperation tool.  It can vary the linear and angular velocity independently which traces out straight or gracefully curved lines, turn in place, reverse, etc.  The minimum linear and angular velocity is zero and the maximum seems to be unbounded, although it is difficult to test because the robot runs into a wall before reaching maximum speed.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab3/sim1.PNG">
                                    <p class="text-left">Upon striking an obstacle, the robot is replaced with a warning sign and will not move or respond to commands.  It must be moved manually with the cursor before it will continue its moving at its previous linear and angular velocity.  The robot can also be moved with the cursor at any time, not just when it strikes a wall.</p>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab3/sim2.PNG">
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 4-->
        <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-labelledby="portfolioModal4Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal4Label">Lab 4: Open-Loop Control</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab4/assembled.jpg" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="text-left">The purpose of this lab is to change from manual to open loop control of the car. At the end of this lab, the car was able to execute a pre-programmed series of moves, using the Artemis board and the motor driver communicating via i2c interface.</p><br>
                                    <h3>Assembly</h3>
                                    <p class="text-left">To begin, I removed the existing circuit board and replaced it with the provided motor driver.  I was able to undo one of the QWIC connectors, feed it through the old hole for the "On" button hole, and reassemble the connector.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab4/oldCircuit.jpg"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab4/newCircuit.jpg">
                                    <p class="text-left">Before closing up the car, I installed the Serial Control Motor Driver library and uploaded an example sketch to verify that I could control each motor individually.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab4/closedCar.jpg">
                                    <h3>Motor Testing</h3>
                                    <p class="text-left">First, I wanted to determine the minimum motor speed command that would actually turn the wheels.  I modified the example sketch to set the motor speed to iterate from 0-255, pausing for a half second in between commands and printing the motor speed to the Serial Monitor.  The wheels would overcome friction and begin spinning reliably at 60-65 LSB/ 255.  Both wheels always started spinning withing 1-2 LSB of eachother, indicating that the motor drive trains are very symmetric and will probably drive straight under open-loop control.</p><br>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/U1asrzKPWNQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">It must be noted that this minimum motor speed was determined from rest with no load.  The actual minimum speed command will be higher if the car were driving on the floor with rolling resistance and lower if the wheels were already spinning.</p><br>
                                    <p class="text-left">I then modified the example sketch to set both wheels in forward drive for one second, pause for one second, then set both wheels in reverse for one second, then pause for another second.  I let the robot run this loop on the tiled floor and observed how straight its trajectory was.</p><br>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/FF5BzxpRQ6E" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">Next, I modified the example sketch with a sequence of motor speeds and delays.  Ideally, the robot will drive forward, trace out a square, drive back to where it started, then spin in place.  With no feedback to correct itself, achieving this ideal trajectory would be impossible, but I just wanted to test how it turns by itself.</p><br>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/q4QOPdmefD8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <p class="text-left">Finally, as suggested on CampusWire, the "PSWC" pins on the Artemis can serve as a power switch. I soldered pins to them and have a removable jumber.  Removing/replacing the jumper to turn the Artemis on/off is far more convenient that plugging and unplugging the battery.  Below is one picture with the jumper in place with the car off and one picture with the jumper removed and the car on.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab4/wJumper.jpg"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab4/noJumper.jpg"><br>
                                    <h3>Simulator Testing</h3>
                                    <p class="text-left">I also did some open-loop testing on the simulator.  The instructions in the lab manual were followed to setup the Jupyter notebook and simulator.  I modified the code to trace a perfect square with the robot.  This was easy because the robot is given precise linear and angular velocity commands.  I made 4 equally straight and long paths interspaced between 4 90 degree turns to form a perfect square and put the robot back where it started.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab4/simCode.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab4/sim1.PNG"><br>
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 5-->
        <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-labelledby="portfolioModal5Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal5Label">Lab 5: Obstacle Avoidance</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab5/robot.jpg" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="text-left">The purpose of this lab is to enable the robot to perform obstacle avoidance. To do this, we first need to equip the robot with distance sensors - the further the robot can see and the more it can trust a sensor reading, the faster it is able to drive. Once you have your sensors working, you will mount them on your robot and attempt fast motion around your room without crashing into obstacles. You will use the VM simulator to implement similar behavior.</p><br>
                                    <h3>Proximity Sensor</h3>
                                    <p class="text-left">First, I installed the library and opened a few example sketches.  The I2C address matched, so I began taking readings immediately.  The sensor measures the light intensity of the returning IR beam it projected into the environment.  Consequentially, it reacts very differently depending on the texture and color of the surface it is seeing.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/proxTest.jpg"><br>
                                    <p class="text-left">I performed some simple tests to observe how well the sensor could be used as a distance sensor.  I recorded the sensor measurements for 7 different materials at 3 fixed distances each.  When comparing measurements at the same distance, glossy, smooth, lighter colored materials like white paper returned the IR light the most while dull, darker materials returned it the least.  Over the 3 distances for each material, the relationship between sensor measurement and distance was more linear for the dull, darker materials than for the glossy, lighter ones.  I also tested the sensor in a dark room, and while shining a flashlight at it, but it had no noticeable effect.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/proxTestData.PNG"><br>
                                    <p class="text-left">The update frequency of this sensor was very quick, coming in at less than one millisecond.  However, the sensor performed poorly for distances further than 200mm and was very noisy for distances under 50mm.  For this reason, I decided against using this sensor for this lab.  At high speeds, the robot would have too much inertia to begin slowing down with 200mm to an object in front of it.  Another reason was the non-linearity of the sensor measurement - distance relationship and the variability of the measurement with respect to material.</p><br>
                                    <p class="text-left">Despite its drawbacks, I see potential in this sensor.  If I attach pieces of white tape radially to the inside of one of the black tires, I may be able to use it as an optical encoder.  There is significant need for a speed sensor and this may suffice, but this will be investigated further in the next lab.</p><br>
                                    <h3>Time of Flight (ToF) Sensor</h3>
                                    <p class="text-left">I followed a similar procedure to test the ToF sensor.  Interestingly, the I2C address on the datasheet (0x52) did not match what the I2C scanner had found (0x29).  Nontheless, I used the scanner address and the sensor worked with no issues.  I soon discovered the range and dependability of this sensor were impressive.</p><br>
                                    <p class="text-left">My range tests for the ToF sensor were performed over 1-5 ft and with only 3 materials.  The sensor was accurate to within an inch and did not depend on the type of material or lighting conditions.  It has an operating range of ~5mm to 6m and updates every 100ms, although I changed this to 50ms for a more reactive robot.  I also tested it with the "Status and Rate" Example Sketch which gave indicators for the quality of the measurement.  I waved a notebook infront of it quickly to simulate a fast moving object, but the sensor rarely returned an invalid measurement.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/TOFTestData.PNG"><br>
                                    <p class="text-left">I also performed the calibration procedure using the example sketch.  The measurement offset was 11mm.  I decided to use this sensor as the primary range finder for the robot because its range, update frequency, and accuracy were suitable obstacle avoidance.</p><br>
                                    <h3>Obstacle Avoidance</h3>
                                    <p class="text-left">After sensor testing was complete, I mounted a small piece of cardboard to the front of the robot with hot glue and tape, then taped both sensors to the front and wired them to the motor driver and the Artemis.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/frontRobot.jpg"><br>
                                    <p class="text-left">The sketch for obstacle avoidance was broken into 5 simple sections: initialize, sense, filter, decide, actuate.</p><br>
                                    <p class="text-left">I combined code snippets from examples and initialized the global variables and constants I would need to decide when an obstacle was too close, how fast to drive, how far away an obstacle was, etc. </p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/code1.PNG"><br>
                                    <p class="text-left">I continually took ToF measurements without clearing the interrupt to save time because it had no noticeable effect on the measurements.  When the measurement was ready, I subtracted the offset from the raw measurement and save it in <code>measToF</code>.  Then, I filtered the measurements using a complimentary filter.  <code>alpha</code> was set to 0.25 because the measurements were very reliable.  In hindsight, I think the system would work without filtering the data.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/code2.PNG"><br>
                                    <p class="text-left">Once the filtered range dropped below <code>closeEnough</code>, the motors were set to turn right.  Otherwise, they were set to move straight ahead.  I tested some methods of avoidance where I would use <code>delay()</code> to perform braking and turning, but none worked as well at higher speeds as the simple approach I settled on.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/code3.PNG"><br>
                                    <p class="text-left">For testing obstacle avoidance, I gradually increased the speed of the robot which presented new problems.  At high speeds, the inertia of the robot was too much for the robot to turn to the right before hitting an obstacle.  I solved this by increasing <code>closeEnough</code>, although it created problems when the robot was in an area where most obstacles around it were closer than this threshold and it would just turn in place.  All told, the final version of this code drove the motors at full speed and usually avoided hitting most obstacles.  To make it better, I would implement the optical encoder with the IR sensor I mentioned and vary <code>closeEnough</code> with the speed of the robot.</p><br>
                                    <div class="embed-container">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9iG7R-XIPAU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div><br>
                                    <h3>Obstacle Avoidance in the Simulator</h3>
                                    <p class="text-left">I used a similar code structure for the simulator.  The simulated range sensor measurements had no noise, so the filtering step was unnecessary and was not included.  The robot needs to turn enough so that when it is done turning and continues forward, its trajectory will encounter an object withing <code>closeEnough</code> before the side of the robot hits a wall.  The robot is a small square, about 0.3m per side.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab5/code4.PNG"><br>
                                    <p class="text-left">I noticed that the robot will hit a wall and stop when it moves along the wall at a shallow angle.  I added a delay after the turn command to lengthen the minimum turn angle when it sees an obstacle.  This did not solve the problem, but certainly helped.  I'm not sure how to implement a perfect obstacle avoidance algorithm without additional sensors.</p><br>
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 6-->
        <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-labelledby="portfolioModal6Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal6Label">Lab 6: IMU and Odometry</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab6/robotSquare.jpg" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="text-left">The purpose of this lab is to setup your IMU (to enable your robot to estimate its orientation). Note that there are several ways to compute these, through this lab you should understand the trade offs of each approach. Once the IMU is implemented, you will mount it on your robot and attempt to do PID control on the rotational speed. Enabling reliable rotation will help you implement navigation and localization in labs 8-9; enabling reliable and slow rotation will help you do TOF rotational scans to compute a map in lab 7.</p><br>
                                    <h3>IMU Setup</h3>
                                    <p class="text-left">First, I secured the IMU to the robot chassis near the center of mass and familiarized myself with the accelerometer, gyroscope, and magnetometer.</p><br>
                                    <p class="text-left">For the accelerometer, I used the equations from class to convert the raw data into pitch and roll.  At {-90, 0, 90} degrees of pitch and roll, the accelerometer estimate was accurate to within 1-2 degrees.  The deviations are a combination of white noise, inaccuracies associated with the accelerometer itself, and the fact that the IMU is not actually oriented at exactly {-90, 0, 90} degrees.  The average deviation for each was almost negligible, but I adjusted the pitch by a 1.25 degree constant offset to bring its average output closer to its expected.  Any acceleration not due to gravity induces large errors in the accelerometer angle estimates.</p><br>
                                    <p class="text-left">I also performed a frequency response for the accelerometer.  I observed the raw, unagitated output and fed it into the Fast Fourier Transform from the lab instructions.  The output showed a small peak near 225 Hz.  Therefore, my low-pass filters for my accelerometer must be designed to reject noise at that frequency.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/accSignal.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/accFourier.PNG"><br>
                                    <p class="text-left">For the gyroscope, I integrated the raw data through time for yaw, pitch, and roll. Since the error in each measurement acculates, each angle estimate drifts by 1-3 degrees per second.  This effect was mitigated by subtracting the average offset of the raw data when the IMU is stationary.  The angles computed from gyroscope data are also more accurate in the short term and less susceptible to noise.</p><br>
                                    <p class="text-left">I created a complimentary filter that is both stable and accurate by combining the estimates from sensor in certain proportions.  The figure below shows the robustness of the filter in comparison to each sensor's individual estimate when agitated in roll.  The green in the complimentary filter estimate, blue is the accelerometer (notice the noisy output), and the red is the gyroscope's estimate (notice the accumulated errors).</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/rollComp.PNG"><br>
                                    <p class="text-left">For the magnetometer, ...</p><br>
                                    <h3>Open-Loop Control</h3>
                                    <p class="text-left">Before implementing feedback control, I performed some open-loop testing to get a better feel for how the robot spins.</p><br>
                                    <p class="text-left">Although I was supposed to use bluetooth for wireless communication, I had several issues with the code and the VM.  I found a workaround by using a telemetry radio connected to the other UART on the Artemis.  This setup allowed me to send messages over <code>Serial1</code> to my desktop where the ground module of the telemetry radio was plugged in.  The serial messages were parsed by Matlab and contained the estimated yaw rate from the gyroscope and the current motor command.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/robotAntennas.jpg">
                                    <p class="text-left">The open-loop script made the motors spin in opposite directions at increasing speed until maximum speed, then decreasing speed until the robot stopped.  By observing the response, I was able to determine the deadband (the motor value below which the motors don't spin) of each motor, hysteresis (the deadband may depend on if the motor command is increasing or decreasing), and the max rotational speed.</p><br>
                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/dRKkWFkehtk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/rampResponse.png"><br>
                                    <p class="text-left">With the ramp response, I estimated that the lowest speed that the open-loop robot would rotate about its own axis was about 360 deg/s.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/steadyOLyaw.png"><br>
                                    <h3>Closed-Loop Control</h3>
                                    <p class="text-left">The structure of the closed-loop script was similar to that of the object-avoidance program: sense, compute, actuate.  The PID library I used in this lab was the same I had used for other projects, so I was comfortable with how it worked.</p><br>
                                    <p class="text-left">I decided to implement a PI controller to avoid the pitfalls associated with the D-term and high frequency noise in the measurement.  To begin, I set the reference point of the controller to 360 deg/s and used low P and I gains.  With little tuning, I was able to get the average of the yaw rate right around 360 deg/s.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/pid1.png"><br>
                                    <p class="text-left">I continued testing and tuning, progressively decreasing the reference yaw rate and increasing the gains appropriately.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/pid2.png"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/pid3.png"><br>
                                    <p class="text-left">This worked until I approached 180 deg/s where the steady state motor command is close enough to the deadband of the right motor for the motor to not spin.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/pid4.png"><br>
                                    <p class="text-left">To attempt to further reduce the minimum reliable yaw rate, I used the same program to turn only one motor (the left, stronger one) while the right motor was set to a low value (40) to avoid braking.  This worked well and I was able to reduce the minimum reliable yaw rate to 120 deg/s (one rotation every 3 seconds).  The downside of this method is the robot no longer rotates about its own axis, but instead moves in a circle (a reliable one) with a radius of about 8 inches.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/pid5.png"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/pid6.png"><br>
                                    <p class="text-left">Below is a video of this program working.  From 0:06 - 0:23 (17s), the robot completes about 6 rotations which is about 1 rotation every 3 seconds which is about 120 deg/s.</p><br>
                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/9OM4GKh18M4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <h3>ToF Sensor Mapping</h3>
                                    <p class="text-left">The intention of controlling the yaw rate is to perform mapping using a ranging sensor. A slower yaw rate is desirable because it will produce a map with finer resolution per rotation and because the uncertainty associated with ranging sensor will be lower.</p><br>
                                    <p class="text-left">The ToF sensor has a minimum sampling time (the time during which the measurement is actually taken) of 20ms in short mode and 33ms otherwise.  At the minimum reliable open-loop yaw rate of 360 deg/s and a distance of 0.5m from a perpendicular wall, the car will rotate ~0.125 rad (~0.207 rad if not in short mode) during the sampling time.  Using trigonometry, the distance between the sensor and the wall changes by almost 4mm (11 mm if not in short mode).  The uncertainty in the measurement is made far worse when the robot is originally oriented at 45 degrees to the wall.  In this case, the distance between the sensor and the wall changes by 108 mm! (208 mm if not in short mode!).</p><br>
                                    <p class="text-left">This level of uncertainty will be difficult to deal with because the ToF sensor flags measurements as invalid if they violate either of the two configurable parameters used to qualify measurements. <code>Sigma</code> represents the estimated standard deviation of the measurement and its default tolerance is 15mm.  <code>Signal Rate</code> represents the amplitude of the reflected signal.</p><br>
                                    <p class="text-left">Assuming the robot is rotating at 120 deg/s about its own axis, the distance between the sensor and the wall changes by only 15 mm (54 mm if not in short mode) in the worst case scenario (45 degrees to the wall). This great reduction in uncertainty makes mapping with the ToF far more feasible.  I expect this will be investigated further in the coming labs.</p><br>
                                    <h3>Odometry and Ground Truth in the Virtual Robot</h3>
                                    <p class="text-left">The remaining section of the lab was very simple. I used the simulator and a plotter to command the robot in an environment, record its pose according to odometry, and compare that to its actual pose.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/sim.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/vmCode.PNG"><br>
                                    <p class="text-left">Clearly, the errors associated with the measurements and the model itself induce deviations in the dead reckoning estimate of the robots position.  These deviations are never corrected to bring the dead reckoning pose estimate back to the true pose, making it impossible to get a reliable pose estimate from odometry alone.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab6/plotter.PNG"><br>
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 7-->
        <div class="portfolio-modal modal fade" id="portfolioModal7" tabindex="-1" role="dialog" aria-labelledby="portfolioModal7Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal7Label">Lab 7: Mapping</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab7/kitchen.jpg" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="text-left">In this lab, I laid the groundwork for implementing grid localization in simulation for the next lab.  I also did some rudimentary mapping with the physical robot and the ToF sensor.</p><br>
                                    <h3>Grid Localization Setup</h3>
                                    <p class="text-left">To begin, I familiarized myself with the classes and functions that would be required to implement a Bayesian Filter in the next lab.  Next, I sent the virtual robot on its pre-planned path through the simulation environment.  Below is figure of the environment and a figure with the true and the odometry estimated trajectory shown in green and blue respectively.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/sim.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/simTraj.PNG"><br>
                                    <p class="text-left">Last, I developed pseudocode for the Bayesian Filter.  The skeleton code provided to us included functions with inputs and outputs to help organize the final implementation.</p><br>
                                    <p class="text-left">Using geometry and the odometry model defined in class, I wrote out the <code>compute_control</code> function to compute the control action sequence that moved the robot from a previous pose to its current one.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/code1.PNG"><br>
                                    <p class="text-left">Using the previous function, I also wrote in the <code>odom_motion_model</code> function that returns the probability that the robot is at a new state given its previous state and the control input over the last timestep.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/code2.PNG"><br>
                                    <p class="text-left">I also wrote the <code>sensor_model</code> function which is responsible for returning the liklihood of a set of observations given what the robot actually observed.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/code3.PNG"><br>
                                    <p class="text-left">For the remaining prediction and update functions, each requires a set of 3 nested <code>for</code> loops to iterate through each grid cell in the 20x20x18 matrix.  For each cell, the belief is predicted using odometry data, then updated using measurement data.  After the completion of the update step, the entire grid must be normalized or else all of the probabilities will converge to zero.  I wrote a short function to do this for a 1-D array as well.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/code4.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/code5.PNG"><br>
                                    <h3>Mapping the Environment</h3>
                                    <p class="text-left">Before beginning any actual mapping, I modified a copy of the bluetooth example to send measurement data over bluetooth to the VM where it could be post-processed into a map.  This new program combined the functionality of the previous labs, including ToF sensor ranging, reading and filtering gyroscope measurements into reliable yaw estimate, using a PI controller to track 120 deg/s of yaw rate with one wheel, and sending ToF measurements and yaw estimates to the VM.  The robot was programmed to drive in one circle while sending data, then stop.</p><br>
                                    <p class="text-left">For the mapping part of the lab, I set up boxes to make an enclosure with a few interesting features.  I made sure to have an obstacle in the middle of the floor in addition to a 'hole' in the wall.  Below is the actual environment as well as a sketch of the environment.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/kitchen.jpg"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/sketch.jpg"><br>
                                    <h3>Mapping with the Robot</h3>
                                    <p class="text-left">The first plots looked better than I expected (I was not expecting them to even vaguely resemble the actual environment).  I thought that the combination of the uncertainty in the yaw estimate, the fact that the robot is not rotating about its own axis, and the sampling frequency of the ToF sensor would make the map unrecognizable, but I was wrong.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/polarMap.png"><br>
                                    <p class="text-left">Next, I post-processed the data in Matlab.  I assumed that the robot moved in a perfect circle with a radius of 6", starting at the top of the circle and moving counterclockwise.  I then transformed the measurements from the sensor frame to the robot frame, and finally to the inertial frame.  I assumed that the interial frame's origin was located where the robot began its trajectory.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/cartMap.png"><br>
                                    <p class="text-left">The results of post-processing were very noticeable.  First, the features of the environment are clear.  I can clearly see corners, straight walls, the obstacle, and the 'hole' in the environment.  Second, the inconsistencies are less noticeable, namely the range discrepancy at 0 degrees.  This point in space is ranged twice; once at the beginning of the robot's turn and once at the end.  Since the robot does not move in a perfect circle, the wall appears to have an object protruding from it when it is infact straight.</p><br>
                                    <p class="text-left">I spot-checked the map with a tape-measure.  Most major walls and features are accurate to within 10cm and are straight with no protrusions.</p><br>
                                    <p class="text-left">I converted the environment into a format the simulator could use for visualization in the plotter tool.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab7/myEnv.PNG"><br>
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 8-->
        <div class="portfolio-modal modal fade" id="portfolioModal8" tabindex="-1" role="dialog" aria-labelledby="portfolioModal8Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal8Label">Lab 8: Localization in Simulation</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab8/Bayes.PNG" alt="" />
                                    <!-- Portfolio Modal - Text-->
                                    <h3>Objective</h3>
                                    <p class="text-left">The purpose of this lab is to implement the Bayesian Grid Localization algorithm we've been discussing in lecture.  The implementation in this lab is a precursor to the localization algorithm that will be used on the physical robot in the next lab.</p><br>
                                    <h3>Implementation</h3>
                                    <p class="text-left">Programming the Bayes filter was simple considering the general structure was already laid out in the previous lab.</p><br>
                                    <p class="text-left">For <code>compute_control</code>, I only had to make a small modification to my previous implementation for it to work.  The angles for turning were not bounded to [-180,180) degrees which could have led to problems around the discontinuity.  I used <code>.normalize_angles()</code> to make it more robust.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/code1.PNG"><br>
                                    <p class="text-left"><code>odom_motion_model</code> didn't require any modifications.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/code2.PNG"><br>
                                    <p class="text-left">For <code>prediction_step</code>, I had the general structure correct, but misunderstood what the inputs and outputs were supposed to be.  We need the previous and current pose from odometry to calculate what control inputs were used over the last timestep using <code>compute_control</code>.  We also need the pose for which we were most confident of in the last timestep, <code>best_prev_belief</code>.  The prediction probability of each cell in <code>bel_bar</code> was updated using <code>odom_motion_model</code> to the probability that we got to each cell's position from <code>best_prev_belief</code> using the control input we used.  Poses around the pose that the robot arrived at from <code>best_prev_belief</code> using the control inputs have high probability in comparison to poses on the other side of the environment.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/code3.PNG"><br>
                                    <p class="text-left"><code>update_step</code> had a similar structure to prediction step. For each of the 18 measurements, we use the actual measurement<code>z_actual</code> to compare to the measurement from each cell.  The belief of pose <code>bel</code> is updated by multiplying the product of the array of measurement probabilities with the <code>bel_bar</code> from the corresponding cell.  Thus, if the actual measurement and the expected measurement are similar AND the <code>bel_bar</code> from that cell is high, then that pose gets high probability.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/code4.PNG"><br>
                                    <p class="text-left">I added a probability tolerance check to reduce computation time.  If any of the elements in the measurement probability array are less than 1e-40, the <code>bel</code> of that cell is set to zero.  The tolerance was chosen heuristically to reduce computation time without ruling out any highly probable poses.  Typically, the algorithm will only have to actually calculate the <code>bel</code> for 100-400 cells instead of the full 7200.</p><br>
                                    <h3>Performance</h3>
                                    <p class="text-left">The filter was able to reliably track the trajectory of the robot.  The entire trajectory took a little under 7 minutes, the extension due to computation time. It took a little over a second to compute each prediction step and a little less than half a second to compute each update step.  The figures below are of the robot in the simulator and the plotter at 3 timesteps in the trajectory.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/sim1.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/plotter1.PNG"><br>
                                    <p class="text-left">The actual trajectory is in green, the odometry estimate in blue, and the Bayes filter in yellow.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/sim2.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/plotter2.PNG"><br>
                                    <p class="text-left">The prediction step belief distribution is shown as shaded cells where the lighter cells have higer probability of the robot being there.  Occasionally, the prediction is completely off due to extremely noisy odometry information.  However, the filter could usually rely on the sensor measurements to retain an acceptable pose estimate.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/sim3.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab8/plotter3.PNG"><br>
                                    <p class="text-left">Over several trials, the filter was able to reliably track the trajectory of the robot with few deviations.</p><br>
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 9-->
        <div class="portfolio-modal modal fade" id="portfolioModal9" tabindex="-1" role="dialog" aria-labelledby="portfolioModal9Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal6Label">Lab 9: Localization</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab6/robotSquare.jpg" alt="" />
                                    <h3>Objective</h3>
                                    <p class="text-left">In the lab, I implemented the full Bayes filter that we have been working on.  This required devloping interfaces between several modules and proved challenging.</p><br>
                                    <h3>Setup</h3>
                                    <p class="text-left">I began with writing a new Arduino sketch to control the robot and communicate with the VM.  Using the example bluetooth sketch as the skeleton, I developed functionality to receive commands from the VM over bluetooth and send data back upon request.</p><br>
                                    <p class="text-left">Upon receiving <code>REQUEST_SCAN</code>, the robot would drive in a counter-clockwise circle at 120 deg/s using PID control while recording ToF measurements (as in the previous labs).  The measurements divided into 18 cells over the 360 degree scan (20 deg per cell) where each cell contained the average of all the ToF measurements corresponding to those 20 degrees.  After completing one circle, the robot sent the 18 element array back to the VM.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/scanCode.PNG"><br>
                                    <p class="text-left"><code>scan_startingYaw</code> is the yaw angle of the robot at the start of a scan (0 deg for the first scan).  For a reason that has yet to be discovered, <code>scan_startingYaw</code> goes to <code>nan</code> in the middle of the second scan.  Thus, the robot only ever does its first update and prediction steps.</p><br>
                                    <p class="text-left">For this lab, I used the proximity sensor as an encoder to keep track of how far the robot moves in straight lines.  The <code>Int</code> pin on the proximity sensor was programmed to change when the proximity reading changed enough.  I painted opposite quarters of the wheel with white acrylic paint to give contrast for the sensor to pick up.  Upon receiving <code>REQUEST_ENCODER</code>, the robot would send the current count of the encoder to the VM which was an indication of how far the robot moved.  I also integrated the accelerometer data (with a low-end cutoff to reduce drift) to fulfill the same role and for comparison.  Since I couldn't get the whole filter working for more than one step, I just used the integrated accelerometer data because of some issues I was having with the proximity sensor.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/encoderCode.PNG"><br>
                                    <p class="text-left">I also included a simple movement function.  Since the movement commands from bluetooth are unsigned bytes, I had to convert them to signed bytes and then scale them appropriately.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/moveCode.PNG"><br>
                                    <p class="text-left">For python, the functions we had to implement ended up being simpler than I expected. <code>get_pose()</code> simply returns the odometry pose (store in a global variable) in the map frame. <code>perform_observation_loop</code> sends a <code>REQUEST_SCAN</code> command to the Artemis and waits for the response.</p><br>
                                    <p class="text-left"><code>set_vel(v,w,t)</code> translates the linear and angular velocity commands <code>v,w</code> to left and right wheel velocity commands, then converts them to unsigned bytes to be sent over bluetooth.  The function then waits the prescribed time <code>t</code>, sends a velocity command to stop the robot, then sends <code>REQUEST_ENCODER</code> to get the odometry data and update the odometry pose.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/code1.PNG"><br>
                                    <h3>Simple Localization</h3>
                                    <p class="text-left">I was able to get the Bluetooth working in the Jupyter notebook after much trouble.  My biggest challenges involved interfacing the synchronous and asynchronous parts of the code because I did not understand how python expected the code to be managed.  Gradually, the functionality of the code came together.  In the end, I could connect the Artemis to the Jupyter notebook with two-way bluetooth communication.</p><br>
                                    <p class="text-left">Using the Bayes Filter module, I call <code>initialize_bayes_filter()</code> and the robot did an observation scan as expected.  The result of the first observation of the filter is shown below where the yellow dot is the filter's belief and the green is the actual position of the robot.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/result1.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/map1.PNG"><br>
                                    <p class="text-left">After the initialization, I called <code>move_robot()</code> to move the robot forward, then turn.  Within <code>move_robot</code>, <code>set_vel(v,w,t)</code> sends the motor commands to the Artemis and updates the odometry pose.</p><br>
                                    <p class="text-left">The resulting change in odometry pose is fed into <code>step_bayes_filter()</code> which wouldn't work properly because of the issues in the Arduino code.</p><br>
                                    <p class="text-left">Even with only the one update step, the filter localizes well and gets the initial position correct to within a few inches.</p><br>
                                    <!-- Portfolio Modal - Text-->
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 10-->
        <div class="portfolio-modal modal fade" id="portfolioModal10" tabindex="-1" role="dialog" aria-labelledby="portfolioModal10Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal10Label">Lab 10: Path Planning and Execution</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab10/mapPic.png" alt="" />
                                    <h3>Objective</h3>
                                    <p class="text-left">In the lab, I implemented the full Bayes filter that we have been working on.  This required devloping interfaces between several modules and proved challenging.</p><br>
                                    <h3>Environment Setup</h3>
                                    <p class="text-left">To begin, I had to set up a new environment for the physical robot as I traveled home for Thanksgiving. I am also in the middle of moving between houses, so I had lots of boxes to make an interesting environment.  After using a measuring tape to map out the environment, I manually imported it into the VM as a list of <code>start_points</code> and <code>end_points</code> and as a binary occupancy grid.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/mapPic.jpg"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/mapDrawing.png"><br>
                                    <p class="text-left">The discretized grid has the same size cells as in previous labs (20cm x 20cm) for consistency.  In the grid below, 1s represent occupied cells and 0s represent open cells.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/binary_grid.PNG"><br>
                                    <h3>Global Planner</h3>
                                    <p class="text-left">Next, I considered options for a global planners. Due to time constraints, implementing my own algorithm was out of the question.  I was familiar with Dijkstra and found an implementation on this <a href="https://github.com/atomoclast/realitybytes_blogposts">Github Page</a>.  The repository also contained an implementation of A* with an example.  Although time was not a huge constraint and the map I was using was not complex, I decided to use the implemntation of A*.  It produced the same shortest path, but used a simple heuristic to guide the search toward the goal instead of doing an exhaustive search.</p><br>
                                    <h3>Local Planner</h3>
                                    <p class="text-left">Next, I intended to use a local planner to have the robot follow a list of waypoints generated by the global planner.  Since the global planner outputs the sequence of cells to get from the start to the goal, there can be a superfluous amount of waypoints along the path.  The actual list of waypoints still contains the start and the goal cells as waypoints, but skips cells according to the parameter <code>cell_step_size</code> (ex. when <code>cell_step_size</code> is 2, the list of waypoints contains every other cell along the path and includes the start and end goal).</p><br>
                                    <h3>Virtual Robot</h3>
                                    <p class="text-left">For the virtual robot, I decided to use a sequence of turn, move, localize to help the robot follow the path.  I used <code>execute_control(rot1, trans, rot2)</code> with rot2 = 0.  I kept the default movement time as 1 second and adjusted the angular and linear velocity to move the robot where it needed to go.  The angular rate is calculated such that the robot faces towards the next waypoint and the linear velocity is calculated such that the robot covers the distance to the waypoint in 1 second.  Once the distance between the robot and the current waypoint is less than the paramter <code>close_enough</code>, the current waypoint is updated to the next waypoint in the list. This loop runs until the current waypoint is the last waypoint in the list, indicating that the robot has reached the goal.</p><br>
                                    <p class="text-left">To test the algorithms, I ran my code on the virtual robot in the environment I set up.  First, the global planner searched for the shortest path from the start (red) to the goal (green) location.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridPath_1.PNG"><br>
                                    <p class="text-left">The blue cells represent the cells that have been searched.  You can see A* searching towards the goal, but also searching around the path.  The path is drawn in red in the final picture.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridSearch_1_1.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridSearch_1_2.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridSearch_1_3_LI.jpg"><br>
                                    <p class="text-left">After the path was determined, I ran the code for the local planner.  The robot was able to localize and control itself well enough to follow the path and reach its goal, although some tuning of <code>cell_step_size</code> and <code>close_enough</code> was required.  In the first figure, the green is the ground truth path of the robot and the yellow is the Bayes filter path.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/plotter_1.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/sim1_start.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/sim1_final.PNG"><br>
                                    <p class="text-left">I also ran another trial with different start and goal positions to check for robustness.  The second run, the robot was also able to successfully navigate to the goal cell.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridPath_2.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridSearch_2_1.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridSearch_2_2.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/gridSearch_2_3_LI.jpg"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/plotter_2.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/sim2_start.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/sim2_final.PNG"><br>
                                    <p class="text-left">For the second trial, the robot got close enough to the center obstacle to hit it, but not stop it completely.  To prevent this on the real robot, I padded ALL obstacles in the binary occupancy grid by 1 cell so that the global planner wouldn't output a path so close to walls.</p><br>
                                    <h3>Physical Robot</h3>
                                    <p class="text-left">Before modifying my code for the physical robot, I looked into improving my observation loop because this lab was being performed on a smooth, uniform hardwood floor instead of a bumpy tiled floor.  I wrapped the front wheels with electrical tape and retuned the PID controller (increased Ki, decreased Kp) to smooth out the response.  After these two changes, the robot could reliably turn with one wheel in place at 90 deg/s, a massive improvement over driving in a large circle at 120 deg/s.  I also discovered that the format of my observation vector was incorrect.  The first element was supposed to be the scan reading from the current heading instead of the scan reading from 0 deg yaw as I had it for the previous lab.  This improved the localization module's ability to track the yaw of the robot using the observation loop.</p><br>
                                    <p class="text-left">Next, I modified how I set movement commands to the robot.  I decided to use a turn, then move sequence at constant speed, adjusting <code>turn_time</code>, <code>move_time</code>, and the direction of the turn (<code>isCC = 1</code> for counter-clockwise turns, 0 otherwise) proportional to how much turning or moving was needed. The parameters that modified how long the robot turned or moved were tuned heuristically.  After the movement was complete, the robot immediately sent the odometry data to the VM for the prediction step.  I also modified the corresponding modules in the Arduino code to turn or move until <code>turn_time</code> or <code>move_time</code> had elapsed.</p><br>
                                    <p class="text-left">Finally, I modified the virtual robot local planner code, replacing much of the initialization with <code>await init_bayes_filter()</code>, replacing <code>execute_control()</code> with <code>await move_robot(move_time, turn_time, isCC)</code>, and replacing the prediction and update steps with <code>await step_bayes_filter()</code>.  The rest of the code involving distance checks to waypoints and deciding how to move to the next waypoint was left the same.</p><br>
                                    <p class="text-left">I tuned the turning and moving parameters first to prevent over/under turning and moving. Once that was functional, I noticed that the robot was better at making small adjustments to its heading than larger ones, so I modified <code>cell_step_size</code> to reduce the number of waypoint in the path and increase the distance between waypoints.  This was also desirable because it reduced the number of scans the robot was required to perform which saved time.  One caveat of reducing the number of waypoints was that it made it possible for the robot to try to take a path through an obstacle.  I solved this either by finding an appropriate number of waypoints for that particular trajectory or by bloating the obstacles in the occupancy grid so that the global planner found a path further away from the obstacles.</p><br>
                                    <p class="text-left">For the first trial on the real robot, I used the same waypoints as in the first trial of the virtual robot.  The robot began to track the waypoints reliably after the tuning I did.  Below are videos of an unsuccessful early attempt and a subsequent successful attempt as well as a figure of the trajectory of the Bayes filter belief.  For the second video, the robot ended up directly over the cross representing the goal.</p><br>
                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/-wRikEZlH4Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/ggjNUj2Nk_Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/real_plotter1.PNG"><br>
                                    <p class="text-left">I also did a test with the robot starting away from its supposed starting position.  The robot is still able to localize and adjust itself to track the waypoints and reach the goal.  The robot ended up just over 12 inches away from the goal.</p><br>
                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/CHjtS2BhP44" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/real_plotter2.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/trial1_finish.jpg"><br>
                                    <p class="text-left">I performed one last trial using different waypoints.  For this trial, I bloated the obstacles in the occupancy grid after the robot kept hitting the boxes in the middle.  The robot ended up just under 6 inches away from the goal.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/bloatedMap.PNG"><br>
                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/87JmbTADgBw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/real_plotter3.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab10/trial2_finish.jpg"><br>
                                    <p class="text-left">The robot was able to plan paths, localize, and execute well considering what sensors were available.  In previous classes, I have solved this path planning and localization problem with far better precision, but it required reliable, complex sensors on a slow robot.  This robot is capable of much faster, albeit more 'sloppy' navigation.</p><br>
                                    <!-- Portfolio Modal - Text-->
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio Modal 11-->
        <div class="portfolio-modal modal fade" id="portfolioModal11" tabindex="-1" role="dialog" aria-labelledby="portfolioModal11Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true"><i class="fas fa-times"></i></span>
                    </button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0" id="portfolioModal11Label">Lab 11: </h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/lab6/robotSquare.jpg" alt="" />
                                    <h3>Objective</h3>
                                    <p class="text-left">In the lab, I implemented the full Bayes filter that we have been working on.  This required devloping interfaces between several modules and proved challenging.</p><br>
                                    <h3>Setup</h3>
                                    <p class="text-left">I began with writing a new Arduino sketch to control the robot and communicate with the VM.  Using the example bluetooth sketch as the skeleton, I developed functionality to receive commands from the VM over bluetooth and send data back upon request.</p><br>
                                    <p class="text-left">Upon receiving <code>REQUEST_SCAN</code>, the robot would drive in a counter-clockwise circle at 120 deg/s using PID control while recording ToF measurements (as in the previous labs).  The measurements divided into 18 cells over the 360 degree scan (20 deg per cell) where each cell contained the average of all the ToF measurements corresponding to those 20 degrees.  After completing one circle, the robot sent the 18 element array back to the VM.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/scanCode.PNG"><br>
                                    <p class="text-left"><code>scan_startingYaw</code> is the yaw angle of the robot at the start of a scan (0 deg for the first scan).  For a reason that has yet to be discovered, <code>scan_startingYaw</code> goes to <code>nan</code> in the middle of the second scan.  Thus, the robot only ever does its first update and prediction steps.</p><br>
                                    <p class="text-left">For this lab, I used the proximity sensor as an encoder to keep track of how far the robot moves in straight lines.  The <code>Int</code> pin on the proximity sensor was programmed to change when the proximity reading changed enough.  I painted opposite quarters of the wheel with white acrylic paint to give contrast for the sensor to pick up.  Upon receiving <code>REQUEST_ENCODER</code>, the robot would send the current count of the encoder to the VM which was an indication of how far the robot moved.  I also integrated the accelerometer data (with a low-end cutoff to reduce drift) to fulfill the same role and for comparison.  Since I couldn't get the whole filter working for more than one step, I just used the integrated accelerometer data because of some issues I was having with the proximity sensor.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/encoderCode.PNG"><br>
                                    <p class="text-left">I also included a simple movement function.  Since the movement commands from bluetooth are unsigned bytes, I had to convert them to signed bytes and then scale them appropriately.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/moveCode.PNG"><br>
                                    <p class="text-left">For python, the functions we had to implement ended up being simpler than I expected. <code>get_pose()</code> simply returns the odometry pose (store in a global variable) in the map frame. <code>perform_observation_loop</code> sends a <code>REQUEST_SCAN</code> command to the Artemis and waits for the response.</p><br>
                                    <p class="text-left"><code>set_vel(v,w,t)</code> translates the linear and angular velocity commands <code>v,w</code> to left and right wheel velocity commands, then converts them to unsigned bytes to be sent over bluetooth.  The function then waits the prescribed time <code>t</code>, sends a velocity command to stop the robot, then sends <code>REQUEST_ENCODER</code> to get the odometry data and update the odometry pose.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/code1.PNG"><br>
                                    <h3>Simple Localization</h3>
                                    <p class="text-left">I was able to get the Bluetooth working in the Jupyter notebook after much trouble.  My biggest challenges involved interfacing the synchronous and asynchronous parts of the code because I did not understand how python expected the code to be managed.  Gradually, the functionality of the code came together.  In the end, I could connect the Artemis to the Jupyter notebook with two-way bluetooth communication.</p><br>
                                    <p class="text-left">Using the Bayes Filter module, I call <code>initialize_bayes_filter()</code> and the robot did an observation scan as expected.  The result of the first observation of the filter is shown below where the yellow dot is the filter's belief and the green is the actual position of the robot.</p><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/result1.PNG"><br>
                                    <img class="img-fluid img-centered" src="assets/img/portfolio/lab9/map1.PNG"><br>
                                    <p class="text-left">After the initialization, I called <code>move_robot()</code> to move the robot forward, then turn.  Within <code>move_robot</code>, <code>set_vel(v,w,t)</code> sends the motor commands to the Artemis and updates the odometry pose.</p><br>
                                    <p class="text-left">The resulting change in odometry pose is fed into <code>step_bayes_filter()</code> which wouldn't work properly because of the issues in the Arduino code.</p><br>
                                    <p class="text-left">Even with only the one update step, the filter localizes well and gets the initial position correct to within a few inches.</p><br>
                                    <!-- Portfolio Modal - Text-->
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>        
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Contact form JS-->
        <script src="assets/mail/jqBootstrapValidation.js"></script>
        <script src="assets/mail/contact_me.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
